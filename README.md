The problems at the heart of information theory:

* How do you discover order among entropy?
* How does your newly acquired bit of knowledge change your perception about the universe of possibilities?
* How do you mathematically embed information in a recoverable manner?
* How do you play games using data structures and algorithms that allow you to faithfully derive information?
* How to capture variance while optimally sacrificing generalizability?
* Where along the dichotamy of being blindfolded and limited omniescence do we find accurate, performant, explainable, and/or fair mathematical compressions?
* How do we make machines that optimally manipulate these mathematical compressions?

These questions are what makes information theorists (whatever that means), in a manner, observational philosophers. These are the latest artisans, the newest watchmakers, the most considerate engine designers. These blackboxes help humans emulate and automate observation, attention, pattern-recognition, and inference-making.

Here are the simplest lessons we've learned:
* That it is reasonable, given that we are not omniescent, to expect temporal and/or spacial relationships to be representative predictors of the composition of reality.
* That cliche, correlation is not causation; that it is unwise to use observation in place of experimentation.
* That algorithms, like humans and every cognitive framework, produce limitation, loopholes, exceptions, and self-deception.
* That the wisest update their beliefs in presence of new information.
* That biases are unavoidable and even useful in face of the alternative- noise.
